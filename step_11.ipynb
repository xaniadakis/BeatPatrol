{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_label_freq, CNNBackbone, ASTBackbone, LSTMBackbone, SpectrogramDataset, CLASS_MAPPING, get_device, train, torch_train_val_split, set_seed, test_model, plot_train_val_losses, get_classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "DATA_PATH = \"/home/alex/Downloads/archive(1)/data/\"\n",
    "RANDOM_SEED = 42\n",
    "DEVICE = get_device()\n",
    "NUM_CATEGORIES = 10\n",
    "cnn_in_channels = 1\n",
    "cnn_filters = [32, 64]\n",
    "cnn_out_feature_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, backbone):\n",
    "        \"\"\"\n",
    "        backbone (nn.Module): The nn.Module to use for spectrogram parsing\n",
    "        num_classes (int): The number of classes\n",
    "        \"\"\"\n",
    "        super(Classifier, self).__init__()\n",
    "        self.backbone = backbone  # An LSTMBackbone or CNNBackbone\n",
    "        self.is_lstm = isinstance(self.backbone, LSTMBackbone)\n",
    "        self.output_layer = nn.Linear(self.backbone.feature_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss() if num_classes > 1 else nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, targets, lengths):\n",
    "        if not self.is_lstm:\n",
    "            feats = self.backbone(x)  \n",
    "        else:\n",
    "            self.backbone(x, lengths)\n",
    "        logits = self.output_layer(feats)\n",
    "        logits = logits.squeeze(-1) if self.criterion.__class__ == nn.modules.loss.MSELoss else logits\n",
    "        loss = self.criterion(logits, targets)\n",
    "        return loss, logits, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_specs_test_data = SpectrogramDataset(DATA_PATH + \"fma_genre_spectrograms/\", class_mapping=CLASS_MAPPING, train=False)\n",
    "mel_specs_test_dl, _ = torch_train_val_split(dataset=mel_specs_test_data, batch_eval=BATCH_SIZE, batch_train=BATCH_SIZE, val_size=.0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m backbone \u001b[38;5;241m=\u001b[39m \u001b[43mCNNBackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_specs_test_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_in_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_out_feature_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/alex/AFA8-7A06/Master/First-Semester/Pattern-Recognition/2nd-Lab-Assignment/Updated/BeatPatrol/utils.py:587\u001b[0m, in \u001b[0;36mCNNBackbone.__init__\u001b[0;34m(self, input_dims, in_channels, filters, feature_size)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    575\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, filters[\u001b[38;5;241m0\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    576\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    577\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    578\u001b[0m     nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    581\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(filters[\u001b[38;5;241m0\u001b[39m], filters[\u001b[38;5;241m1\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    582\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    583\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    584\u001b[0m         nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 587\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(filters[\u001b[38;5;241m1\u001b[39m], \u001b[43mfilters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    588\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m    589\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    590\u001b[0m         nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    593\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(filters[\u001b[38;5;241m2\u001b[39m], filters[\u001b[38;5;241m3\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    594\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m3\u001b[39m]),\n\u001b[1;32m    595\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    596\u001b[0m         nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    598\u001b[0m shape_after_convs \u001b[38;5;241m=\u001b[39m [input_dims[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filters)), input_dims[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filters))]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "backbone = CNNBackbone(input_dims=mel_specs_test_data.feats[0].shape, in_channels=cnn_in_channels, filters=cnn_filters, feature_size=cnn_out_feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weight_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m backbone \u001b[38;5;241m=\u001b[39m CNNBackbone(input_dims\u001b[38;5;241m=\u001b[39mmel_specs_test_data\u001b[38;5;241m.\u001b[39mfeats[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, in_channels\u001b[38;5;241m=\u001b[39mcnn_in_channels, filters\u001b[38;5;241m=\u001b[39mcnn_filters, feature_size\u001b[38;5;241m=\u001b[39mcnn_out_feature_size)  \n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Classifier(num_classes\u001b[38;5;241m=\u001b[39mNUM_CATEGORIES, backbone\u001b[38;5;241m=\u001b[39mbackbone)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pattrec1/lib/python3.9/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/pattrec1/lib/python3.9/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/pattrec1/lib/python3.9/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weight_path'"
     ]
    }
   ],
   "source": [
    "backbone = CNNBackbone(input_dims=mel_specs_test_data.feats[0].shape, in_channels=cnn_in_channels, filters=cnn_filters, feature_size=cnn_out_feature_size)  \n",
    "model = Classifier(num_classes=NUM_CATEGORIES, backbone=backbone).to(device=DEVICE)\n",
    "model.load_state_dict(torch.load(\"weight_path\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(RANDOM_SEED)\n\u001b[0;32m----> 2\u001b[0m backbone \u001b[38;5;241m=\u001b[39m \u001b[43mCNNBackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_specs_test_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_in_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_out_feature_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m set_seed(RANDOM_SEED)  \n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Classifier(num_classes\u001b[38;5;241m=\u001b[39mNUM_CATEGORIES, backbone\u001b[38;5;241m=\u001b[39mbackbone)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m/media/alex/AFA8-7A06/Master/First-Semester/Pattern-Recognition/2nd-Lab-Assignment/Updated/BeatPatrol/utils.py:587\u001b[0m, in \u001b[0;36mCNNBackbone.__init__\u001b[0;34m(self, input_dims, in_channels, filters, feature_size)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    575\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, filters[\u001b[38;5;241m0\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    576\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    577\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    578\u001b[0m     nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    581\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(filters[\u001b[38;5;241m0\u001b[39m], filters[\u001b[38;5;241m1\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    582\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    583\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    584\u001b[0m         nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 587\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(filters[\u001b[38;5;241m1\u001b[39m], \u001b[43mfilters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    588\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m    589\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    590\u001b[0m         nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    593\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(filters[\u001b[38;5;241m2\u001b[39m], filters[\u001b[38;5;241m3\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    594\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m*\u001b[39m filters[\u001b[38;5;241m3\u001b[39m]),\n\u001b[1;32m    595\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    596\u001b[0m         nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    598\u001b[0m shape_after_convs \u001b[38;5;241m=\u001b[39m [input_dims[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filters)), input_dims[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filters))]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "backbone = CNNBackbone(input_dims=mel_specs_test_data.feats[0].shape, in_channels=cnn_in_channels, filters=cnn_filters, feature_size=cnn_out_feature_size)\n",
    "set_seed(RANDOM_SEED)  \n",
    "model = Classifier(num_classes=NUM_CATEGORIES, backbone=backbone).to(device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latents(model, test_dl, device):\n",
    "    model.to(device)\n",
    "    features = []\n",
    "    for inputs, targets, lenghts in test_dl:\n",
    "        inputs, targets, lenghts = inputs.to(device), targets.to(device), lenghts.to(device)\n",
    "        print(inputs.device)\n",
    "        _, _, feats = model(inputs, targets, lenghts)\n",
    "        features.append(feats)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_latents(\u001b[43mmodel\u001b[49m, mel_specs_test_dl, DEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "features = extract_latents(model, mel_specs_test_dl, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattrec1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
