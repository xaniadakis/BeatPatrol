{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import CNNBackbone, MultiTaskDataset, ASTBackbone, get_device, SpectrogramDataset, CLASS_MAPPING, torch_train_val_split, Classifier, train, set_seed, plot_train_val_losses, test_model, get_regression_report, create_folder\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_PATH = \"/home/alex/Downloads/archive(1)/data/\"\n",
    "EPOCHS = 100\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 42\n",
    "RNN_HIDDEN_SIZE = 64\n",
    "NUM_CATEGORIES = 1\n",
    "cnn_in_channels = 1\n",
    "cnn_filters = [32, 64, 128, 256]\n",
    "cnn_out_feature_size = 256\n",
    "DEVICE = get_device()\n",
    "\n",
    "\n",
    "create_folder(\"model_weights\"), create_folder(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=1)\n",
    "energy_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=2)\n",
    "dancability_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.578, 0.973, 0.873],\n",
       "       [0.839, 0.782, 0.655],\n",
       "       [0.587, 0.956, 0.204],\n",
       "       ...,\n",
       "       [0.337, 0.592, 0.316],\n",
       "       [0.536, 0.404, 0.366],\n",
       "       [0.477, 0.949, 0.431]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_task_labels = []\n",
    "for valence_y, energy_y, dancability_y in zip(valence_data.labels, energy_data.labels, dancability_data.labels):\n",
    "    multi_task_labels.append((valence_y, energy_y, dancability_y))\n",
    "np.array(multi_task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_dataset = MultiTaskDataset(features=valence_data.feats, labels=np.array(multi_task_labels))\n",
    "dataloader = DataLoader(multi_task_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskClassifier(nn.Module):\n",
    "    def __init__(self, num_tasks, backbone, task_feature_sizes):\n",
    "        \"\"\"\n",
    "        num_tasks (int): The number of tasks (e.g., 3 metrics)\n",
    "        backbone (nn.Module): The shared backbone (CNNBackbone or LSTMBackbone)\n",
    "        task_feature_sizes (list of int): Output sizes for each task\n",
    "        \"\"\"\n",
    "        super(MultiTaskClassifier, self).__init__()\n",
    "        self.backbone = backbone  # Shared backbone\n",
    "        \n",
    "        # Separate output layers for each task\n",
    "        self.output_layers = nn.ModuleList([\n",
    "            nn.Linear(self.backbone.feature_size, task_feature_sizes[i]) for i in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # Criterion for each task\n",
    "        self.criterions = [nn.MSELoss() for _ in range(num_tasks)]  # Regression losses\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        \"\"\"\n",
    "        x: Input features\n",
    "        targets: List of target tensors for each task\n",
    "        lengths: Sequence lengths (for LSTM inputs)\n",
    "        \"\"\"\n",
    "        # Shared feature extraction\n",
    "        feats = self.backbone(x)\n",
    "        \n",
    "        # Task-specific outputs each element holds the predictions for the corresponding head\n",
    "        logits = [output_layer(feats) for output_layer in self.output_layers]\n",
    "        logits = [logit.squeeze(-1) for logit in logits]\n",
    "        # Compute losses for each task\n",
    "        losses = [criterion(logits[i], targets[:, i]) for i, criterion in enumerate(self.criterions)]\n",
    "        \n",
    "        # Weighted sum of losses (equal weight for simplicity; can be tuned)\n",
    "        total_loss = sum(losses)\n",
    "        \n",
    "        return total_loss, losses, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTotal Loss at training set: 0.4920566976070404\n",
      "\t0.271818071603775, 0.1185767725110054, 0.10166185349225998\n",
      "Epoch 5\n",
      "\tTotal Loss at training set: 8.623558044433594\n",
      "\t7.062807083129883, 1.3222465515136719, 0.23850411176681519\n",
      "Epoch 10\n",
      "\tTotal Loss at training set: 0.47748029232025146\n",
      "\t0.30532971024513245, 0.09234171360731125, 0.07980887591838837\n",
      "Epoch 15\n",
      "\tTotal Loss at training set: 0.21627843379974365\n",
      "\t0.06962907314300537, 0.12688522040843964, 0.019764143973588943\n",
      "Epoch 20\n",
      "\tTotal Loss at training set: 0.1924845427274704\n",
      "\t0.07543405890464783, 0.11010538041591644, 0.006945108529180288\n",
      "Epoch 25\n",
      "\tTotal Loss at training set: 0.17525197565555573\n",
      "\t0.07651092112064362, 0.08794593811035156, 0.0107951108366251\n",
      "Epoch 30\n",
      "\tTotal Loss at training set: 0.1587393581867218\n",
      "\t0.0710899606347084, 0.08248107880353928, 0.00516832061111927\n",
      "Epoch 35\n",
      "\tTotal Loss at training set: 0.1441487818956375\n",
      "\t0.06292903423309326, 0.08041144907474518, 0.0008082911954261363\n",
      "Epoch 40\n",
      "\tTotal Loss at training set: 0.1402556151151657\n",
      "\t0.06230969727039337, 0.07698062807321548, 0.0009652756853029132\n",
      "Epoch 45\n",
      "\tTotal Loss at training set: 0.13484162092208862\n",
      "\t0.05855449289083481, 0.07553613185882568, 0.0007509925635531545\n",
      "Epoch 50\n",
      "\tTotal Loss at training set: 0.12828080356121063\n",
      "\t0.053058914840221405, 0.07419509440660477, 0.0010267978068441153\n",
      "Epoch 55\n",
      "\tTotal Loss at training set: 0.12065792828798294\n",
      "\t0.0457782968878746, 0.07343405485153198, 0.0014455795753747225\n",
      "Epoch 60\n",
      "\tTotal Loss at training set: 0.10964753478765488\n",
      "\t0.03489941358566284, 0.07247113436460495, 0.0022769849747419357\n",
      "Epoch 65\n",
      "\tTotal Loss at training set: nan\n",
      "\tnan, nan, nan\n",
      "Epoch 70\n",
      "\tTotal Loss at training set: nan\n",
      "\tnan, nan, nan\n"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "backbone = ASTBackbone(\n",
    "    fstride=10,                     \n",
    "    tstride=10,                   \n",
    "    input_fdim=dancability_data[0][0].shape[1],      \n",
    "    input_tdim=dancability_data[0][0].shape[0],     \n",
    "    imagenet_pretrain=False,      \n",
    "    model_size='small224',          \n",
    "    feature_size=1    \n",
    ")\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "model = MultiTaskClassifier(num_tasks=3, backbone=backbone, task_feature_sizes=[1, 1, 1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)  # Every 10 epochs, reduce LR by factor of 0.7\n",
    "inputs, targets, lengths = next(iter(dataloader))\n",
    "inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "for epoch in range(80):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses, logits = model(inputs.float(), targets.float())\n",
    "    loss.backward()\n",
    "    # clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch == 0 or (epoch+1)%5 == 0:\n",
    "        loss1, loss2, loss3 = losses\n",
    "        print(f'Epoch {epoch+1}\\n\\tTotal Loss at training set: {loss.item()}\\n\\t{loss1.item()}, {loss2.item()}, {loss3.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTotal Loss at training set: 0.004171649085150824\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 4314.486393871307\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 4455.163112621307\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 4806.403425121307\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 5998.0998140101965\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 7151.465091787974\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 8052.096897343529\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 8486.65175845464\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 8594.07314734353\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 8600.06731780794\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 8671.255238814883\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 8930.74989159266\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9261.431697148217\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9495.900377703772\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9738.337391592662\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9834.048398537107\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9862.144431523217\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9883.29841155794\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 9924.937074752384\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10016.385963641273\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10095.089661557939\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10188.080026141273\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10224.581940203772\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10244.940045672523\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10260.516938033634\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10262.248288266923\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10270.604846426646\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10292.153418475256\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10309.0787863138\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10336.880533275604\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10352.39970211241\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10378.778821036021\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10400.114860532549\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10407.592606843313\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10416.794697772131\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10424.75169429991\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10426.291061704424\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10429.888091869354\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10434.08746008767\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10436.92684539583\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10439.374628056421\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10444.985295916664\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10451.561282244787\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10458.255625235239\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10463.036958785588\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10465.61202524609\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10466.628535255855\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10467.459280969832\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10469.36557993359\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10470.724292227427\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10473.56439476649\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10475.222412480249\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10479.714652063582\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10482.325815258026\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10484.39655093511\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10486.958111807506\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10487.640713600582\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10487.9946199184\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10488.519095965492\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10489.675305682289\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10491.699469093746\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10493.9357140753\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10497.248868100907\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10498.092351648542\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10498.523653731876\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10498.915186833276\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10499.31824476242\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10499.76410308626\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10502.277170306312\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10503.034620867835\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10504.21307844162\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10505.539689634112\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10506.17943898307\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10506.704297008515\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10507.256930472056\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10508.721189505259\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10509.794430065154\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10510.104221019745\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10511.545173168182\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10512.042907509274\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10512.224349549611\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10512.455651565127\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10512.864475858476\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10513.14515141593\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10513.42684358173\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10513.912230680255\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10514.32098224004\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10514.640564657848\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10515.100677776336\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10515.48282160229\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10516.368569079505\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10516.542174070146\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10516.785654176076\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10517.289444730546\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10518.322213895586\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10518.669648452335\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10518.943310070037\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10519.112413158417\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10519.414911123911\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10519.624911666446\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10519.815469934676\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10520.106307404836\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10520.345241154564\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10520.61549428092\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10521.16632062912\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10521.371538999345\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10521.731887340546\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10521.872191532982\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10522.045290728674\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10522.1441221767\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10522.382147621578\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10522.842023313311\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10523.018158974117\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10523.410965913137\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10523.587352191078\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10523.769777626461\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10524.03862552007\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10524.685063915253\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10526.488341638777\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10526.985142650605\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10527.3406778272\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10527.810526460013\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10528.297979802026\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10528.819201691946\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10529.16159898334\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10529.695322738224\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10530.225520831214\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10530.61223602083\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10531.191228398216\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10531.425388724008\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10531.789888710446\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10532.001957344479\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10533.120906861623\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10533.443106399112\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10534.216196134355\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10534.372691190507\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10535.019677410126\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10535.27227133645\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10536.016915094588\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10536.21925426695\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10536.939400073157\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10537.7753542561\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10537.963024578094\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10538.125195174747\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10538.505598447588\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10538.90993580924\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10539.20466152403\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10539.397125377654\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10539.68832956738\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10540.006110668182\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10540.4556194369\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10540.61837922626\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10540.922620559268\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10541.230141175589\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10541.673920913272\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10542.600912558237\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10543.0338143052\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10543.478499838511\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10543.833642050426\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10544.19183239195\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10544.54133597268\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10544.71940773646\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10544.975822531382\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10545.246466452281\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10545.486007823944\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10545.71034898334\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10548.829470890892\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10549.032999163734\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10549.939577871959\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10550.283168671926\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10550.733098854489\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10551.263775666555\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10551.446229805417\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10552.17087729348\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10552.364406939612\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10552.837197466957\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10553.48155673345\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10554.666385156843\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10555.033528881073\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10555.366155122121\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10555.9236208068\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10556.750649005042\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10556.830195123885\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10558.026153511471\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10558.423800911374\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10560.076503141192\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10560.567510051727\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10560.834035930633\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10561.507270522647\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10561.670618707869\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10561.94328040017\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10562.26128950119\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10562.464413399166\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10562.931381816865\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10563.560014222463\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10563.831032068465\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10564.347728879717\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10564.75310139762\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10565.129852428436\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10566.36852186203\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10566.751614106497\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10567.75786851459\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10568.106217717064\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10568.916522214678\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10569.98814690272\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10570.42530802197\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10571.025518796709\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10571.29652001063\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10571.843323735131\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10572.182072209253\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10572.701555987464\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10573.103718734317\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10573.380325349171\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10573.779250248803\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10574.970537141164\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10576.044617816078\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10578.293083527882\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10578.87909209993\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10580.364231666988\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10581.003694828882\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10584.751956818898\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10585.367350292207\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10588.416458093856\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10588.854462345971\n",
      "Epoch 1\n",
      "\tTotal Loss at training set: 10589.91483452479\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 1.2668950737847222\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 3.0036735026041668\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 4.2536703830295135\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 4.839381713867187\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 7.517749769422743\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 8.873525865342883\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 9.539067111545139\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 10.633469509548611\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 13.362161458333333\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 16.130525716145833\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 18.018805338541668\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 20.628095703125\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 23.8444580078125\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 24.94955756293403\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 27.510320366753472\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 28.35356194390191\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 33.04434644911024\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 35.349173109266495\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 37.01740281846788\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 37.62965935601129\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 39.119096476236976\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 39.74678812662761\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 41.41401658799913\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 42.99131964789497\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 44.09545552571615\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 44.577881503634984\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 46.15630679660373\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 48.76197438557943\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 49.34803178575304\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 52.131248745388454\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 56.38348290337456\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 58.5265330335829\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 63.349786071777345\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 66.27946190728082\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 67.37660396999783\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 67.58161100599501\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 68.99198576185438\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 71.04490215725369\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 71.7302076043023\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 73.94763354831272\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 74.6554615105523\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 75.47722746107313\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 77.259136912028\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 77.85499601576063\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 79.37317364162868\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 80.54096199883355\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 82.0412838575575\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 84.81620790269639\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 85.29178821139865\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 86.45556506686741\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 86.93358745998806\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 88.49852859497071\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 89.65126486884223\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 90.4641808573405\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 91.72561952379014\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 92.65240432739257\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 93.92710091484918\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 95.38851042005751\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 97.66571826510959\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 99.26329327053494\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 101.43629321628147\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 102.09545282999674\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 103.3776357184516\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 103.64599248250326\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 104.85124408298068\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 105.1910695224338\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 105.97075017293294\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 107.2074991692437\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 110.50131373087565\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 113.15046493530274\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 115.40183714124892\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 120.21397581312391\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 124.63428234524197\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 125.80195242987739\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 128.2042544047038\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 129.79436562432184\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 132.27672510782878\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 137.4459932284885\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 139.65891911824545\n",
      "Epoch 2\n",
      "\tTotal Loss at training set: 141.03433551364475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTotal Loss at training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example training loop\n",
    "set_seed(RANDOM_SEED)\n",
    "backbone = CNNBackbone(valence_data[0][0].shape, cnn_in_channels, cnn_filters, cnn_out_feature_size)\n",
    "set_seed(RANDOM_SEED)\n",
    "model = MultiTaskClassifier(num_tasks=3, backbone=backbone, task_feature_sizes=[1, 1, 1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "backbone.train()\n",
    "model.to(DEVICE)\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.\n",
    "    for inputs, targets, _ in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        loss, losses, logits = model(inputs.float(), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}\\n\\tTotal Loss at training set: {running_loss / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattrec1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
