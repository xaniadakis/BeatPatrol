{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import MultiTaskDataset, MultiTaskClassifier, ASTBackbone, get_device, SpectrogramDataset, CLASS_MAPPING, set_seed, plot_train_val_losses, create_folder, train_multi_task_learning\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "DATA_PATH = \"/home/alex/Downloads/archive(1)/data/\"\n",
    "EPOCHS = 100\n",
    "LR = 1e-2\n",
    "BATCH_SIZE = 2\n",
    "RANDOM_SEED = 42\n",
    "NUM_CATEGORIES = 1\n",
    "DEVICE = get_device()\n",
    "\n",
    "\n",
    "create_folder(\"model_weights\"), create_folder(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=1)\n",
    "energy_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=2)\n",
    "dancability_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.578, 0.973, 0.873],\n",
       "       [0.839, 0.782, 0.655],\n",
       "       [0.587, 0.956, 0.204],\n",
       "       ...,\n",
       "       [0.337, 0.592, 0.316],\n",
       "       [0.536, 0.404, 0.366],\n",
       "       [0.477, 0.949, 0.431]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_task_labels = []\n",
    "for valence_y, energy_y, dancability_y in zip(valence_data.labels, energy_data.labels, dancability_data.labels):\n",
    "    multi_task_labels.append((valence_y, energy_y, dancability_y))\n",
    "np.array(multi_task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_dataset = MultiTaskDataset(features=valence_data.feats, labels=np.array(multi_task_labels))\n",
    "\n",
    "train_size = int(0.8 * len(multi_task_dataset))\n",
    "val_size = len(multi_task_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(multi_task_dataset, [train_size, val_size])\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTotal Loss at training set: 0.4920566976070404\n",
      "\t0.271818071603775, 0.1185767725110054, 0.10166185349225998\n",
      "Epoch 5\n",
      "\tTotal Loss at training set: 8.623558044433594\n",
      "\t7.062807083129883, 1.3222465515136719, 0.23850411176681519\n",
      "Epoch 10\n",
      "\tTotal Loss at training set: 0.47748029232025146\n",
      "\t0.30532971024513245, 0.09234171360731125, 0.07980887591838837\n",
      "Epoch 15\n",
      "\tTotal Loss at training set: 0.21627843379974365\n",
      "\t0.06962907314300537, 0.12688522040843964, 0.019764143973588943\n",
      "Epoch 20\n",
      "\tTotal Loss at training set: 0.1924845427274704\n",
      "\t0.07543405890464783, 0.11010538041591644, 0.006945108529180288\n",
      "Epoch 25\n",
      "\tTotal Loss at training set: 0.17525197565555573\n",
      "\t0.07651092112064362, 0.08794593811035156, 0.0107951108366251\n",
      "Epoch 30\n",
      "\tTotal Loss at training set: 0.1587393581867218\n",
      "\t0.0710899606347084, 0.08248107880353928, 0.00516832061111927\n",
      "Epoch 35\n",
      "\tTotal Loss at training set: 0.1441487818956375\n",
      "\t0.06292903423309326, 0.08041144907474518, 0.0008082911954261363\n",
      "Epoch 40\n",
      "\tTotal Loss at training set: 0.1402556151151657\n",
      "\t0.06230969727039337, 0.07698062807321548, 0.0009652756853029132\n",
      "Epoch 45\n",
      "\tTotal Loss at training set: 0.13484162092208862\n",
      "\t0.05855449289083481, 0.07553613185882568, 0.0007509925635531545\n",
      "Epoch 50\n",
      "\tTotal Loss at training set: 0.12828080356121063\n",
      "\t0.053058914840221405, 0.07419509440660477, 0.0010267978068441153\n"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "backbone = ASTBackbone(\n",
    "    fstride=10,                     \n",
    "    tstride=10,                   \n",
    "    input_fdim=dancability_data[0][0].shape[1],      \n",
    "    input_tdim=dancability_data[0][0].shape[0],     \n",
    "    imagenet_pretrain=False,      \n",
    "    model_size='small224',          \n",
    "    feature_size=1    \n",
    ")\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "model = MultiTaskClassifier(num_tasks=3, backbone=backbone, task_feature_sizes=[1, 1, 1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)  # Every 10 epochs, reduce LR by factor of 0.7\n",
    "inputs, targets, lengths = next(iter(train_dl))\n",
    "inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses, logits = model(inputs.float(), targets.float())\n",
    "    loss.backward()\n",
    "    # clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch == 0 or (epoch+1)%5 == 0:\n",
    "        loss1, loss2, loss3 = losses\n",
    "        print(f'Epoch {epoch+1}\\n\\tTotal Loss at training set: {loss.item()}\\n\\t{loss1.item()}, {loss2.item()}, {loss3.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for model ast_multi_task_best...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m     16\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39mEPOCHS, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_multi_task_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mast_multi_task_best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/alex/AFA8-7A06/Master/First-Semester/Pattern-Recognition/2nd-Lab-Assignment/Updated/BeatPatrol/utils.py:859\u001b[0m, in \u001b[0;36mtrain_multi_task_learning\u001b[0;34m(epochs, device, train_dl, val_dl, model, optimizer, scheduler, save_path)\u001b[0m\n\u001b[1;32m    857\u001b[0m early_stopper \u001b[38;5;241m=\u001b[39m EarlyStopper(model, save_path, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining started for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs:\n\u001b[1;32m    860\u001b[0m     avg_train_total_loss, avg_train_valence_loss, avg_train_energy_loss, avg_train_dance_loss \u001b[38;5;241m=\u001b[39m multi_task_learning_one_epoch(device, train_dl, model, optimizer, scheduler)\n\u001b[1;32m    861\u001b[0m     avg_val_total_loss, avg_val_valence_loss, avg_val_energy_loss, avg_val_dance_loss \u001b[38;5;241m=\u001b[39m eval_multi_task(model, val_dl, device)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "backbone = ASTBackbone(\n",
    "    fstride=10,                     \n",
    "    tstride=10,                   \n",
    "    input_fdim=dancability_data[0][0].shape[1],      \n",
    "    input_tdim=dancability_data[0][0].shape[0],     \n",
    "    imagenet_pretrain=False,      \n",
    "    model_size='small224',          \n",
    "    feature_size=1    \n",
    ")\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "model = MultiTaskClassifier(num_tasks=3, backbone=backbone, task_feature_sizes=[1, 1, 1])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-10)\n",
    "\n",
    "train_multi_task_learning(epochs=EPOCHS, \n",
    "                          device=DEVICE, \n",
    "                          train_dl=train_dl, \n",
    "                          val_dl=val_dl, \n",
    "                          model=model, \n",
    "                          optimizer=optimizer, \n",
    "                          scheduler=scheduler, \n",
    "                          save_path=\"ast_multi_task_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattrec1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
