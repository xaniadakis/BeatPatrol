{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import CNNBackbone, MultiTaskDataset, ASTBackbone, get_device, SpectrogramDataset, CLASS_MAPPING, torch_train_val_split, Classifier, train, set_seed, plot_train_val_losses, test_model, get_regression_report, create_folder\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_PATH = \"/home/alex/Downloads/archive(1)/data/\"\n",
    "EPOCHS = 100\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 42\n",
    "RNN_HIDDEN_SIZE = 64\n",
    "NUM_CATEGORIES = 1\n",
    "cnn_in_channels = 1\n",
    "cnn_filters = [32, 64, 128, 256]\n",
    "cnn_out_feature_size = 256\n",
    "DEVICE = get_device()\n",
    "\n",
    "\n",
    "create_folder(\"model_weights\"), create_folder(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=1)\n",
    "energy_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=2)\n",
    "dancability_data = SpectrogramDataset(DATA_PATH + \"multitask_dataset/\", class_mapping=CLASS_MAPPING, train=True, regression=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.578, 0.973, 0.873],\n",
       "       [0.839, 0.782, 0.655],\n",
       "       [0.587, 0.956, 0.204],\n",
       "       ...,\n",
       "       [0.337, 0.592, 0.316],\n",
       "       [0.536, 0.404, 0.366],\n",
       "       [0.477, 0.949, 0.431]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_task_labels = []\n",
    "for valence_y, energy_y, dancability_y in zip(valence_data.labels, energy_data.labels, dancability_data.labels):\n",
    "    multi_task_labels.append((valence_y, energy_y, dancability_y))\n",
    "np.array(multi_task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_dataset = MultiTaskDataset(features=valence_data.feats, labels=np.array(multi_task_labels))\n",
    "dataloader = DataLoader(multi_task_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskClassifier(nn.Module):\n",
    "    def __init__(self, num_tasks, backbone, task_feature_sizes):\n",
    "        \"\"\"\n",
    "        num_tasks (int): The number of tasks (e.g., 3 metrics)\n",
    "        backbone (nn.Module): The shared backbone (CNNBackbone or LSTMBackbone)\n",
    "        task_feature_sizes (list of int): Output sizes for each task\n",
    "        \"\"\"\n",
    "        super(MultiTaskClassifier, self).__init__()\n",
    "        self.backbone = backbone  # Shared backbone\n",
    "        \n",
    "        # Separate output layers for each task\n",
    "        self.output_layers = nn.ModuleList([\n",
    "            nn.Linear(self.backbone.feature_size, task_feature_sizes[i]) for i in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # Criterion for each task\n",
    "        self.criterions = [nn.MSELoss() for _ in range(num_tasks)]  # Regression losses\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        \"\"\"\n",
    "        x: Input features\n",
    "        targets: List of target tensors for each task\n",
    "        lengths: Sequence lengths (for LSTM inputs)\n",
    "        \"\"\"\n",
    "        # Shared feature extraction\n",
    "        feats = self.backbone(x)\n",
    "        \n",
    "        # Task-specific outputs each element holds the predictions for the corresponding head\n",
    "        logits = [output_layer(feats) for output_layer in self.output_layers]\n",
    "        logits = [logit.squeeze(-1) for logit in logits]\n",
    "        # Compute losses for each task\n",
    "        losses = [criterion(logits[i], targets[:, i]) for i, criterion in enumerate(self.criterions)]\n",
    "        \n",
    "        # Weighted sum of losses (equal weight for simplicity; can be tuned)\n",
    "        total_loss = sum(losses)\n",
    "        \n",
    "        return total_loss, losses, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTotal Loss at training set: 0.4920566976070404\n",
      "\t0.271818071603775, 0.1185767725110054, 0.10166185349225998\n",
      "Epoch 5\n",
      "\tTotal Loss at training set: 8.623558044433594\n",
      "\t7.062807083129883, 1.3222465515136719, 0.23850411176681519\n",
      "Epoch 10\n",
      "\tTotal Loss at training set: 0.47748029232025146\n",
      "\t0.30532971024513245, 0.09234171360731125, 0.07980887591838837\n",
      "Epoch 15\n",
      "\tTotal Loss at training set: 0.21627843379974365\n",
      "\t0.06962907314300537, 0.12688522040843964, 0.019764143973588943\n",
      "Epoch 20\n",
      "\tTotal Loss at training set: 0.1924845427274704\n",
      "\t0.07543405890464783, 0.11010538041591644, 0.006945108529180288\n",
      "Epoch 25\n",
      "\tTotal Loss at training set: 0.17525197565555573\n",
      "\t0.07651092112064362, 0.08794593811035156, 0.0107951108366251\n",
      "Epoch 30\n",
      "\tTotal Loss at training set: 0.1587393581867218\n",
      "\t0.0710899606347084, 0.08248107880353928, 0.00516832061111927\n",
      "Epoch 35\n",
      "\tTotal Loss at training set: 0.1441487818956375\n",
      "\t0.06292903423309326, 0.08041144907474518, 0.0008082911954261363\n",
      "Epoch 40\n",
      "\tTotal Loss at training set: 0.1402556151151657\n",
      "\t0.06230969727039337, 0.07698062807321548, 0.0009652756853029132\n",
      "Epoch 45\n",
      "\tTotal Loss at training set: 0.13484162092208862\n",
      "\t0.05855449289083481, 0.07553613185882568, 0.0007509925635531545\n",
      "Epoch 50\n",
      "\tTotal Loss at training set: 0.12828080356121063\n",
      "\t0.053058914840221405, 0.07419509440660477, 0.0010267978068441153\n"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "backbone = ASTBackbone(\n",
    "    fstride=10,                     \n",
    "    tstride=10,                   \n",
    "    input_fdim=dancability_data[0][0].shape[1],      \n",
    "    input_tdim=dancability_data[0][0].shape[0],     \n",
    "    imagenet_pretrain=False,      \n",
    "    model_size='small224',          \n",
    "    feature_size=1    \n",
    ")\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "model = MultiTaskClassifier(num_tasks=3, backbone=backbone, task_feature_sizes=[1, 1, 1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)  # Every 10 epochs, reduce LR by factor of 0.7\n",
    "inputs, targets, lengths = next(iter(dataloader))\n",
    "inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses, logits = model(inputs.float(), targets.float())\n",
    "    loss.backward()\n",
    "    # clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch == 0 or (epoch+1)%5 == 0:\n",
    "        loss1, loss2, loss3 = losses\n",
    "        print(f'Epoch {epoch+1}\\n\\tTotal Loss at training set: {loss.item()}\\n\\t{loss1.item()}, {loss2.item()}, {loss3.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskClassifier(\n",
       "  (backbone): ASTBackbone(\n",
       "    (v): DistilledVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       "      (head_dist): Linear(in_features=384, out_features=1000, bias=True)\n",
       "    )\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=384, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layers): ModuleList(\n",
       "    (0-2): 3 x Linear(in_features=1, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "backbone = ASTBackbone(\n",
    "    fstride=10,                     \n",
    "    tstride=10,                   \n",
    "    input_fdim=dancability_data[0][0].shape[1],      \n",
    "    input_tdim=dancability_data[0][0].shape[0],     \n",
    "    imagenet_pretrain=False,      \n",
    "    model_size='small224',          \n",
    "    feature_size=1    \n",
    ")\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "model = MultiTaskClassifier(num_tasks=3, backbone=backbone, task_feature_sizes=[1, 1, 1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "model.train()\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561/561 [12:26<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\t Average Total Loss:  0.43\n",
      "\t Average Valence Loss:  0.26\n",
      "\t Average Energy Loss:  0.11\n",
      "\t Average Danceability Loss:  0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 111/561 [02:23<09:43,  1.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m curr_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m curr_valence_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     21\u001b[0m curr_energy_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_losses, valence_losses = [], []\n",
    "energy_losses, dance_losses = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    curr_total_loss = 0.\n",
    "    curr_valence_loss = 0.\n",
    "    curr_energy_loss = 0.\n",
    "    curr_dance_loss = 0.\n",
    "    for inputs, targets, _ in tqdm(dataloader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss, losses, logits = model(inputs.float(), targets.float())\n",
    "        loss.backward()\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        curr_total_loss += loss.item()\n",
    "        curr_valence_loss += losses[0].item()\n",
    "        curr_energy_loss += losses[1].item()\n",
    "        curr_dance_loss += losses[2].item()\n",
    "    total_losses.append(curr_total_loss / len(dataloader))\n",
    "    valence_losses.append(curr_valence_loss / len(dataloader))\n",
    "    energy_losses.append(curr_energy_loss / len(dataloader))\n",
    "    dance_losses.append(curr_dance_loss / len(dataloader))\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f\"\\t Average Total Loss: {curr_total_loss / len(dataloader): .2f}\")\n",
    "    print(f\"\\t Average Valence Loss: {curr_valence_loss / len(dataloader): .2f}\")\n",
    "    print(f\"\\t Average Energy Loss: {curr_energy_loss / len(dataloader): .2f}\")\n",
    "    print(f\"\\t Average Danceability Loss: {curr_dance_loss / len(dataloader): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattrec1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
