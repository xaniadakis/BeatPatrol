{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T19:42:54.606979Z",
     "start_time": "2024-12-05T19:42:54.432199Z"
    }
   },
   "source": [
    "from utils import SpectrogramDataset, torch_train_val_split, ASTBackbone, get_regression_report,\\\n",
    "    CLASS_MAPPING, Classifier, Regressor, train, get_device, free_gpu_memory, test_model, plot_train_val_losses\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# DATA_PATH = \"/home/alex/Downloads/archive(1)/data/\"\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"data/\")\n",
    "model_weights_path = os.path.join(os.getcwd(), \"model_weights/\")\n",
    "assets_path = os.path.join(os.getcwd(), \"assets/\")\n",
    "\n",
    "mel_specs_path = DATA_PATH + \"fma_genre_spectrograms/\"\n",
    "beat_mel_specs_path = DATA_PATH + \"fma_genre_spectrograms_beat/\"\n",
    "multitask_path = DATA_PATH + \"multitask_dataset/\"\n",
    "os.makedirs(model_weights_path, exist_ok=True)\n",
    "os.makedirs(assets_path, exist_ok=True)\n",
    "\n",
    "EPOCHS = 100\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "VAL_SIZE = .2\n",
    "RANDOM_SEED = 42\n",
    "NUM_CATEGORIES = 10\n",
    "AST_MODEL_SIZES = ['tiny224', 'small224', 'base224', 'base384']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Detected GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Free GPU Memory: {free_gpu_memory():.2f}%\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No compatible GPU detected.\")\n",
    "DEVICE = get_device()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Free GPU Memory: 96.95%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T19:44:15.285201Z",
     "start_time": "2024-12-05T19:42:55.604597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "mel_specs_data = SpectrogramDataset(mel_specs_path, class_mapping=CLASS_MAPPING, train=True)\n",
    "mel_specs_train_dl, mel_specs_val_dl = torch_train_val_split(\n",
    "    dataset=mel_specs_data,\n",
    "    batch_eval=BATCH_SIZE,\n",
    "    batch_train=BATCH_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "x_b1, _, _ = next(iter(mel_specs_train_dl))\n",
    "input_shape = x_b1[0].shape\n",
    "\n",
    "# init AST Backbone and classifier\n",
    "model_size = AST_MODEL_SIZES[0]\n",
    "backbone = ASTBackbone(\n",
    "    fstride=10,\n",
    "    tstride=10,\n",
    "    input_fdim=input_shape[1],\n",
    "    input_tdim=input_shape[0],\n",
    "    imagenet_pretrain=True,\n",
    "    model_size=model_size,\n",
    "    feature_size=NUM_CATEGORIES,  \n",
    ")\n",
    "model = Classifier(NUM_CATEGORIES, backbone).to(DEVICE)\n",
    "genre_optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Train the model on Spectrograms\n",
    "train_losses, val_losses = train(model, mel_specs_train_dl, \n",
    "                                 mel_specs_val_dl, genre_optimizer, EPOCHS, device=DEVICE)\n",
    "\n",
    "# Save pretrained weights\n",
    "pretrained_weights = model_size+\"_ast_spectrogram_pretraining.pth\"\n",
    "torch.save(model.state_dict(), pretrained_weights)"
   ],
   "id": "c3176807b6f939f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|\u001B[32m          \u001B[0m| 1/100 [01:17<2:07:31, 77.29s/epoch, Epoch=1, Train Loss=2.0755, Val Loss=1.8398, Time (Train)=62.84s, Time (Val)=6.50s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m genre_optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLR)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Train the model on Spectrograms\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m train_losses, val_losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmel_specs_train_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mmel_specs_val_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenre_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Save pretrained weights\u001B[39;00m\n\u001B[1;32m     35\u001B[0m pretrained_weights \u001B[38;5;241m=\u001B[39m model_size\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_ast_spectrogram_pretraining.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/Documents/DSML/PatternRecognition/BeatPatrol/utils.py:503\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, optimizer, epochs, save_path, device, overfit_batch, regression_flag)\u001B[0m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m epoch_bar:\n\u001B[1;32m    502\u001B[0m     start_train \u001B[38;5;241m=\u001B[39m time()\n\u001B[0;32m--> 503\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregression_flag\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m     end_train \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m    505\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "File \u001B[0;32m~/Documents/DSML/PatternRecognition/BeatPatrol/utils.py:419\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, train_loader, optimizer, device, regression_flag)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;66;03m# optimizer step\u001B[39;00m\n\u001B[1;32m    418\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m--> 419\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    421\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)    \n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m avg_loss\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T19:32:33.247729811Z",
     "start_time": "2024-12-05T18:54:14.957123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fine-Tuning for Valence Regression\n",
    "valence_data = SpectrogramDataset(\n",
    "    multitask_path,\n",
    "    class_mapping=CLASS_MAPPING,\n",
    "    train=True,\n",
    "    regression=1,  # Regression task for valence\n",
    ")\n",
    "valence_train_dl, valence_val_dl = torch_train_val_split(\n",
    "    dataset=valence_data,\n",
    "    batch_eval=BATCH_SIZE,\n",
    "    batch_train=BATCH_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize model for regression\n",
    "pretrained_backbone = ASTBackbone(\n",
    "    fstride=10,\n",
    "    tstride=10,\n",
    "    input_fdim=valence_data[0][0].shape[1],\n",
    "    input_tdim=valence_data[0][0].shape[0],\n",
    "    imagenet_pretrain=False,\n",
    "    model_size=model_size,\n",
    "    feature_size=1,  # Single output for regression\n",
    ")\n",
    "pretrained_model = Regressor(pretrained_backbone).to(DEVICE)\n",
    "\n",
    "# Load pretrained weights for fine-tuning\n",
    "pretrained_model.backbone.load_state_dict(torch.load(pretrained_weights, weights_only=True), strict=False)\n",
    "\n",
    "# Freeze all but the last 2 Transformer layers\n",
    "pretrained_backbone.freeze_layers(unfrozen_layers=2)\n",
    "\n",
    "# Verify which parameters are frozen\n",
    "# for name, param in pretrained_model.named_parameters():\n",
    "#     print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n",
    "\n",
    "# Define optimizer after freezing layers to avoid updating frozen parameters\n",
    "valence_optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, pretrained_model.parameters()),  # Only trainable params\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "# Fine-tune the model for Valence\n",
    "finetuned_train_losses, finetuned_val_losses = train(\n",
    "    pretrained_model,\n",
    "    valence_train_dl,\n",
    "    valence_val_dl,\n",
    "    valence_optimizer,\n",
    "    EPOCHS,\n",
    "    device=DEVICE,\n",
    "    regression_flag=True\n",
    ")\n",
    "\n",
    "# Save fine-tuned weights\n",
    "torch.save(pretrained_model.state_dict(), \"ast_valence_finetuning.pth\")"
   ],
   "id": "ba325536270266bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.v.cls_token: Trainable\n",
      "backbone.v.pos_embed: Trainable\n",
      "backbone.v.dist_token: Trainable\n",
      "backbone.v.patch_embed.proj.weight: Trainable\n",
      "backbone.v.patch_embed.proj.bias: Trainable\n",
      "backbone.v.blocks.0.norm1.weight: Frozen\n",
      "backbone.v.blocks.0.norm1.bias: Frozen\n",
      "backbone.v.blocks.0.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.0.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.0.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.0.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.0.norm2.weight: Frozen\n",
      "backbone.v.blocks.0.norm2.bias: Frozen\n",
      "backbone.v.blocks.0.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.0.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.0.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.0.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.1.norm1.weight: Frozen\n",
      "backbone.v.blocks.1.norm1.bias: Frozen\n",
      "backbone.v.blocks.1.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.1.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.1.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.1.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.1.norm2.weight: Frozen\n",
      "backbone.v.blocks.1.norm2.bias: Frozen\n",
      "backbone.v.blocks.1.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.1.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.1.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.1.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.2.norm1.weight: Frozen\n",
      "backbone.v.blocks.2.norm1.bias: Frozen\n",
      "backbone.v.blocks.2.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.2.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.2.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.2.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.2.norm2.weight: Frozen\n",
      "backbone.v.blocks.2.norm2.bias: Frozen\n",
      "backbone.v.blocks.2.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.2.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.2.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.2.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.3.norm1.weight: Frozen\n",
      "backbone.v.blocks.3.norm1.bias: Frozen\n",
      "backbone.v.blocks.3.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.3.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.3.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.3.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.3.norm2.weight: Frozen\n",
      "backbone.v.blocks.3.norm2.bias: Frozen\n",
      "backbone.v.blocks.3.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.3.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.3.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.3.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.4.norm1.weight: Frozen\n",
      "backbone.v.blocks.4.norm1.bias: Frozen\n",
      "backbone.v.blocks.4.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.4.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.4.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.4.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.4.norm2.weight: Frozen\n",
      "backbone.v.blocks.4.norm2.bias: Frozen\n",
      "backbone.v.blocks.4.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.4.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.4.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.4.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.5.norm1.weight: Frozen\n",
      "backbone.v.blocks.5.norm1.bias: Frozen\n",
      "backbone.v.blocks.5.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.5.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.5.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.5.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.5.norm2.weight: Frozen\n",
      "backbone.v.blocks.5.norm2.bias: Frozen\n",
      "backbone.v.blocks.5.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.5.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.5.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.5.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.6.norm1.weight: Frozen\n",
      "backbone.v.blocks.6.norm1.bias: Frozen\n",
      "backbone.v.blocks.6.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.6.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.6.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.6.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.6.norm2.weight: Frozen\n",
      "backbone.v.blocks.6.norm2.bias: Frozen\n",
      "backbone.v.blocks.6.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.6.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.6.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.6.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.7.norm1.weight: Frozen\n",
      "backbone.v.blocks.7.norm1.bias: Frozen\n",
      "backbone.v.blocks.7.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.7.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.7.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.7.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.7.norm2.weight: Frozen\n",
      "backbone.v.blocks.7.norm2.bias: Frozen\n",
      "backbone.v.blocks.7.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.7.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.7.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.7.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.8.norm1.weight: Frozen\n",
      "backbone.v.blocks.8.norm1.bias: Frozen\n",
      "backbone.v.blocks.8.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.8.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.8.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.8.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.8.norm2.weight: Frozen\n",
      "backbone.v.blocks.8.norm2.bias: Frozen\n",
      "backbone.v.blocks.8.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.8.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.8.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.8.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.9.norm1.weight: Frozen\n",
      "backbone.v.blocks.9.norm1.bias: Frozen\n",
      "backbone.v.blocks.9.attn.qkv.weight: Frozen\n",
      "backbone.v.blocks.9.attn.qkv.bias: Frozen\n",
      "backbone.v.blocks.9.attn.proj.weight: Frozen\n",
      "backbone.v.blocks.9.attn.proj.bias: Frozen\n",
      "backbone.v.blocks.9.norm2.weight: Frozen\n",
      "backbone.v.blocks.9.norm2.bias: Frozen\n",
      "backbone.v.blocks.9.mlp.fc1.weight: Frozen\n",
      "backbone.v.blocks.9.mlp.fc1.bias: Frozen\n",
      "backbone.v.blocks.9.mlp.fc2.weight: Frozen\n",
      "backbone.v.blocks.9.mlp.fc2.bias: Frozen\n",
      "backbone.v.blocks.10.norm1.weight: Trainable\n",
      "backbone.v.blocks.10.norm1.bias: Trainable\n",
      "backbone.v.blocks.10.attn.qkv.weight: Trainable\n",
      "backbone.v.blocks.10.attn.qkv.bias: Trainable\n",
      "backbone.v.blocks.10.attn.proj.weight: Trainable\n",
      "backbone.v.blocks.10.attn.proj.bias: Trainable\n",
      "backbone.v.blocks.10.norm2.weight: Trainable\n",
      "backbone.v.blocks.10.norm2.bias: Trainable\n",
      "backbone.v.blocks.10.mlp.fc1.weight: Trainable\n",
      "backbone.v.blocks.10.mlp.fc1.bias: Trainable\n",
      "backbone.v.blocks.10.mlp.fc2.weight: Trainable\n",
      "backbone.v.blocks.10.mlp.fc2.bias: Trainable\n",
      "backbone.v.blocks.11.norm1.weight: Trainable\n",
      "backbone.v.blocks.11.norm1.bias: Trainable\n",
      "backbone.v.blocks.11.attn.qkv.weight: Trainable\n",
      "backbone.v.blocks.11.attn.qkv.bias: Trainable\n",
      "backbone.v.blocks.11.attn.proj.weight: Trainable\n",
      "backbone.v.blocks.11.attn.proj.bias: Trainable\n",
      "backbone.v.blocks.11.norm2.weight: Trainable\n",
      "backbone.v.blocks.11.norm2.bias: Trainable\n",
      "backbone.v.blocks.11.mlp.fc1.weight: Trainable\n",
      "backbone.v.blocks.11.mlp.fc1.bias: Trainable\n",
      "backbone.v.blocks.11.mlp.fc2.weight: Trainable\n",
      "backbone.v.blocks.11.mlp.fc2.bias: Trainable\n",
      "backbone.v.norm.weight: Trainable\n",
      "backbone.v.norm.bias: Trainable\n",
      "backbone.v.head.weight: Trainable\n",
      "backbone.v.head.bias: Trainable\n",
      "backbone.v.head_dist.weight: Trainable\n",
      "backbone.v.head_dist.bias: Trainable\n",
      "backbone.mlp_head.0.weight: Trainable\n",
      "backbone.mlp_head.0.bias: Trainable\n",
      "backbone.mlp_head.1.weight: Trainable\n",
      "backbone.mlp_head.1.bias: Trainable\n",
      "output_layer.weight: Trainable\n",
      "output_layer.bias: Trainable\n",
      "Training started for model checkpoint...\n",
      "Epoch 1/100\n",
      "\tAverage Training Loss: 0.10367603090682388 (30.85s)\n",
      "\tAverage Validation Loss: 0.0691038669486131(3.09s)\n",
      "Epoch 2/100\n",
      "\tAverage Training Loss: 0.06610885951503188 (29.75s)\n",
      "\tAverage Validation Loss: 0.06965146759258849(3.08s)\n",
      "Epoch 3/100\n",
      "\tAverage Training Loss: 0.06670822976239488 (30.76s)\n",
      "\tAverage Validation Loss: 0.07176643556782178(3.08s)\n",
      "Epoch 4/100\n",
      "\tAverage Training Loss: 0.06726914504130857 (29.68s)\n",
      "\tAverage Validation Loss: 0.07179881479325038(3.21s)\n",
      "Epoch 5/100\n",
      "\tAverage Training Loss: 0.06652602476718943 (29.52s)\n",
      "\tAverage Validation Loss: 0.07073308926607881(3.33s)\n",
      "Epoch 6/100\n",
      "\tAverage Training Loss: 0.06681704344804835 (30.55s)\n",
      "\tAverage Validation Loss: 0.07514296638380204(3.11s)\n",
      "Early Stopping was activated.\n",
      "Training has been completed.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = train_losses.extend(finetuned_train_losses)\n",
    "val_losses = val_losses.extend(finetuned_val_losses)\n",
    "plot_train_val_losses(train_losses, val_losses, save_title=f\"assets/finetuned_ast_train_val_losses.png\")"
   ],
   "id": "503444aeab5cb6ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T19:32:33.248537166Z",
     "start_time": "2024-12-05T19:00:54.181947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_true, y_pred, spear_corrs = test_model(pretrained_model, valence_val_dl, DEVICE, regression_flag=True)\n",
    "\n",
    "print(\"Fine-Tuned AST Regressor on Valence Dataset\")\n",
    "get_regression_report(y_pred, y_true, spear_corrs)"
   ],
   "id": "d872892657eb97a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned AST Regressor on Valence Dataset\n",
      "\tSpearman Correlation: 0.3119\n",
      "\tMSE: 0.0751\n",
      "\tMAE: 0.2299\n",
      "\tRMSE: 0.2741\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
